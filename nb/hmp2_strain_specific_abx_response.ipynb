{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "\n",
    "_os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfacts as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import scipy as sp\n",
    "from operator import eq\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sample_entropy(w, discretized=False):\n",
    "    if discretized:\n",
    "        gen = w.genotype.discretized().data\n",
    "    else:\n",
    "        gen = w.genotype.data\n",
    "        \n",
    "    com = w.community.data\n",
    "    depth = w.metagenotype.total_counts()\n",
    "        \n",
    "    return ((sf.math.binary_entropy(com @ gen) * depth).sum(\"position\") / depth.sum(\"position\")).rename(\"entropy\")\n",
    "\n",
    "def max_strain_depth(w):\n",
    "    return (w.community.data * w.metagenotype.mean_depth()).max('sample').rename('depth')\n",
    "\n",
    "def total_strain_depth(w):\n",
    "    return (w.community.data * w.metagenotype.mean_depth()).sum('sample').rename('depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgen = pd.read_table('meta/mgen.tsv', index_col='library_id')\n",
    "preparation = pd.read_table('meta/preparation.tsv', index_col='preparation_id')\n",
    "stool = pd.read_table('meta/stool.tsv', index_col='stool_id')\n",
    "visit = pd.read_table('meta/visit.tsv', index_col='visit_id')\n",
    "subject = pd.read_table('meta/subject.tsv', index_col='subject_id')\n",
    "\n",
    "mgen_meta = (\n",
    "    mgen\n",
    "    .join(preparation.drop(columns='library_type'), on='preparation_id')\n",
    "    .join(stool, on='stool_id')\n",
    "    .join(visit, on='visit_id', rsuffix='_')\n",
    "    .join(subject, on='subject_id')\n",
    ")\n",
    "\n",
    "assert not any(mgen_meta.subject_id.isna())\n",
    "\n",
    "# meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_taxonomy = pd.read_table('ref/gtpro/species_taxonomy_ext.tsv', names=['genome_id', 'species_id', 'taxonomy_string']).assign(species_id=lambda x: x.species_id.astype(str)).set_index('species_id')[['taxonomy_string']].assign(taxonomy_split=lambda x: x.taxonomy_string.str.split(';'))\n",
    "\n",
    "for level_name, level_number in [('p__', 1), ('c__', 2), ('o__', 3), ('f__', 4), ('g__', 5), ('s__', 6)]:\n",
    "    species_taxonomy = species_taxonomy.assign(**{level_name: species_taxonomy.taxonomy_split.apply(lambda x: x[level_number])}) \n",
    "species_taxonomy = species_taxonomy.drop(columns=['taxonomy_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_week = (\n",
    "    visit\n",
    "    .join(subject, on='subject_id')\n",
    "    .reset_index()\n",
    "    .dropna(subset=['subject_id', 'week_number'])\n",
    "    .groupby(['subject_id', 'week_number'])\n",
    "    .apply(lambda d: d.loc[d.notna().sum(1).sort_values().index[-1]])\n",
    "    .assign(subject_week_id=lambda x: x.subject_id + '_' + x.week_number.astype(int).astype(str))\n",
    "    .set_index('subject_week_id')\n",
    "    .join(stool.groupby('visit_id').fecal_calprotectin.mean(), on='visit_id')\n",
    ")\n",
    "\n",
    "mgen_to_subject_week = mgen_meta.dropna(subset=['week_number']).apply(lambda x: x.subject_id + '_' + str(int(x.week_number)), axis=1).rename('subject_week_id')\n",
    "mgen_to_subject_week\n",
    "\n",
    "#.groupby(['subject_id', 'week_number']).visit_id.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_id = '101493'\n",
    "species_taxonomy.loc[species_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all_species_depth = (\n",
    "    pd.read_table('data/hmp2.a.r.proc.gtpro.species_depth.tsv', index_col=['sample', 'species_id'])\n",
    "    .squeeze()\n",
    "    .unstack('species_id', fill_value=0)\n",
    ")\n",
    "_all_species_depth.columns = _all_species_depth.columns.astype(str)\n",
    "_all_species_rabund = _all_species_depth.divide(_all_species_depth.sum(1), axis=0)\n",
    "_species_depth = _all_species_depth[species_id]\n",
    "\n",
    "all_species_depth = _all_species_depth.groupby(mgen_to_subject_week).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metagenotype_stem = f'data/sp-{species_id}.hmp2.a.r.proc.gtpro.filt-poly05-cvrg05'\n",
    "metagenotype = sf.Metagenotype.load(f'{metagenotype_stem}.mgen.nc')\n",
    "world_path = f'{metagenotype_stem}.fit-sfacts10-s75-g10000-seed0.world.nc'\n",
    "world = sf.World.load(world_path)\n",
    "print(world_path)\n",
    "\n",
    "meta = mgen_meta.loc[world.sample].sort_values(['subject_id', 'visit_date'])\n",
    "metagenotype = metagenotype.sel(sample=meta.index)\n",
    "world = world.sel(sample=meta.index)\n",
    "\n",
    "same_subject = sp.spatial.distance.pdist(meta.subject_id.values.reshape((-1, 1)), metric=eq).astype(bool)\n",
    "\n",
    "n_position_ss = min(world.sizes['position'], 1000)\n",
    "w_ss = world.random_sample(position=n_position_ss).sel(strain=world.community.max(\"sample\") > 0.01)\n",
    "\n",
    "print(metagenotype.sizes['sample'])\n",
    "print(metagenotype.sizes['position'], world.sizes['position'])\n",
    "print(w_ss.sizes['strain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "subject_index_mod = {k: v for k, v in zip(meta.subject_id.unique(), cycle(range(20)))}\n",
    "diagnosis_map = dict(zip(meta.ibd_diagnosis.unique(), range(100)))\n",
    "site_map = dict(zip(meta.site.unique(), range(100)))\n",
    "\n",
    "\n",
    "\n",
    "sf.plot.plot_community(\n",
    "    w_ss.sel(sample=meta.index),\n",
    "    col_colors_func=lambda w: xr.Dataset(dict(\n",
    "        site=meta.loc[w.sample].site.map(site_map),\n",
    "        diagnosis=meta.loc[w.sample].ibd_diagnosis.map(diagnosis_map),\n",
    "        subject=meta.loc[w.sample].subject_id.map(subject_index_mod),\n",
    "    )),\n",
    "    row_col_annotation_cmap=mpl.cm.tab20,\n",
    "    row_linkage_func=lambda w: w.genotype.discretized().linkage(dim=\"strain\"),\n",
    "    col_linkage_func=lambda w: w.community.linkage(\"sample\"),\n",
    "    scalex=0.05,\n",
    "    xticklabels=0,\n",
    "    # col_cluster=False,\n",
    "    # norm=mpl.colors.PowerNorm(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot.plot_metagenotype(\n",
    "    w_ss.sel(sample=meta.index),\n",
    "    col_colors_func=lambda w: xr.Dataset(dict(\n",
    "        site=meta.loc[w.sample].site.map(site_map),\n",
    "        diagnosis=meta.loc[w.sample].ibd_diagnosis.map(diagnosis_map),\n",
    "        subject=meta.loc[w.sample].subject_id.map(subject_index_mod),\n",
    "    )),\n",
    "    col_linkage_func=lambda w: w.community.linkage(\"sample\"),\n",
    "    row_col_annotation_cmap=mpl.cm.tab20,\n",
    "    scalex=0.05,\n",
    "    xticklabels=0,\n",
    "    # col_cluster=False,\n",
    "    # norm=mpl.colors.PowerNorm(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_strain_depth = (world.community.to_series().unstack().T * _species_depth).T.fillna(0)\n",
    "_strain_depth_other = _species_depth - _strain_depth.sum(1)\n",
    "_strain_depth = _strain_depth.assign(other=_strain_depth_other)\n",
    "\n",
    "strain_depth = _strain_depth.groupby(mgen_to_subject_week).sum()\n",
    "\n",
    "strain_rabund = strain_depth.divide(all_species_depth.sum(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_pairs(df, status, keep):\n",
    "    curr_subject_week_id = None\n",
    "    curr_week_number = 0\n",
    "    curr_perturbation_status = None\n",
    "    intervening = False\n",
    "    out = []\n",
    "    for next_subject_week_id, x in df.sort_values(['subject_id', 'week_number']).iterrows():\n",
    "        next_perturbation_status = status(x)\n",
    "        if not keep(x):\n",
    "            if next_perturbation_status == curr_perturbation_status:\n",
    "                continue\n",
    "            else:\n",
    "                intervening = True\n",
    "                continue\n",
    "        else:\n",
    "            out.append((\n",
    "                curr_subject_week_id,\n",
    "                curr_perturbation_status,\n",
    "                next_subject_week_id,\n",
    "                next_perturbation_status,\n",
    "                x.week_number - curr_week_number,\n",
    "                intervening,\n",
    "            ))\n",
    "            curr_subject_week_id = next_subject_week_id\n",
    "            curr_perturbation_status = next_perturbation_status\n",
    "            curr_week_number = x.week_number\n",
    "            intervening = False\n",
    "            continue\n",
    "    return (\n",
    "        pd.DataFrame(out, columns=['left_subject_week_id', 'left_status', 'right_subject_week_id', 'right_status', 'week_delta', 'intervening'])\n",
    "        .set_index('left_subject_week_id', drop=False)\n",
    "        .rename_axis(index='subject_week_id')\n",
    "        .dropna(subset=['left_subject_week_id'])\n",
    "    )\n",
    "    \n",
    "perturbation_pairs = (\n",
    "    subject_week.assign(has_mgen=lambda x: x.index.isin(strain_rabund.index))\n",
    "    .groupby('subject_id')\n",
    "    .apply(_collect_pairs, status=lambda x: x.status_antibiotics, keep=lambda x: x.has_mgen)\n",
    "    .reset_index('subject_id')\n",
    "    .assign(transition=lambda x: x.left_status.astype(str).str[0] + x.right_status.astype(str).str[0])\n",
    ")\n",
    "\n",
    "pseudo = 1e-4\n",
    "_rabund = strain_rabund\n",
    "_ratio = {}\n",
    "for _tax_id in tqdm(_rabund.columns):\n",
    "    _ratio[_tax_id] = (\n",
    "        perturbation_pairs\n",
    "        .assign(\n",
    "            left_value=lambda x: _rabund[_tax_id].loc[x.left_subject_week_id].values + pseudo,\n",
    "            right_value=lambda x: _rabund[_tax_id].loc[x.right_subject_week_id].values + pseudo,\n",
    "        )\n",
    "        .assign(log_ratio=lambda x: np.log(x.right_value) - np.log(x.left_value))\n",
    "        .log_ratio\n",
    "    )\n",
    "strain_log_ratio = pd.DataFrame(_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_log_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = strain_log_ratio.groupby(perturbation_pairs.transition).mean().T.sort_values('FT')\n",
    "\n",
    "d1 = d\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter('FT', 'TF', data=d1, color='grey', s=1)\n",
    "sns.regplot('FT', 'TF', data=d1, scatter=False, ax=ax)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "for ax, strain_id in zip(axs.flatten(), [d1.index[0], d1.index[-1]]):\n",
    "    d2 = perturbation_pairs.assign(\n",
    "                left_value=lambda x: strain_rabund[strain_id].loc[x.left_subject_week_id].values,\n",
    "                right_value=lambda x: strain_rabund[strain_id].loc[x.right_subject_week_id].values,\n",
    "                ratio=strain_log_ratio[strain_id],\n",
    "            )\n",
    "    ax.set_title(strain_id)\n",
    "    sns.stripplot('transition', 'ratio', data=d2, ax=ax)\n",
    "\n",
    "print(sp.stats.pearsonr(d1['FT'], d1['TF']))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "thresh = 1e-5\n",
    "_present = (_rabund > thresh)\n",
    "_m = subject_week.loc[_rabund.index]\n",
    "\n",
    "x_subject = dmatrix('subject_id - 1', data=_m, return_type='dataframe')\n",
    "x_abx = _m.status_antibiotics.astype(int)\n",
    "y = _present.loc[:, _present.sum(0) > 3]\n",
    "\n",
    "m = len(y.columns)\n",
    "n = len(y.index)\n",
    "r = len(x_subject.columns)\n",
    "\n",
    "print(n, m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(_present.sum(1), bins=np.arange(_present.sum(1).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model0:\n",
    "    _y = pm.Data('_y', y)\n",
    "    _x_subject = pm.Data('_x_subject', x_subject)\n",
    "    _x_abx = pm.Data('_x_abx', x_abx)\n",
    "\n",
    "    beta_subject = pm.Normal('beta_subject', sigma=10., shape=(r, m))\n",
    "    beta_abx_strain = pm.Normal('beta_abx_strain', sigma=10., shape=(1, m))\n",
    "    beta_abx_pooled = pm.Normal('beta_abx_pooled', sigma=10.)\n",
    "    \n",
    "    logit_prob = (\n",
    "        (_x_subject @ beta_subject)\n",
    "        + (_x_abx.reshape((n, 1)) @ beta_abx_strain)\n",
    "        + (_x_abx.reshape((n, 1)) * beta_abx_pooled)\n",
    "    )\n",
    "    prob = pm.math.invlogit(logit_prob)\n",
    "    alpha = pm.Lognormal('alpha', mu=2, sigma=1)  # FIXME: Increase sigma\n",
    "    obs = pm.BetaBinomial('obs', alpha=prob * alpha, beta=(1 - prob) * alpha, n=1, observed=_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model0:\n",
    "    trace = pm.sample(return_inferencedata=True, chains=12, cores=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.log(trace.posterior.alpha.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(trace.posterior.beta_abx_pooled.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_beta = {}\n",
    "for i in range(m):\n",
    "    median_beta[y.columns[i]] = np.median(trace.posterior.beta_abx_strain[:,:,0,i].values.flatten())\n",
    "    sns.kdeplot(trace.posterior.beta_abx_strain[:,:,0,i].values.flatten())\n",
    "median_beta = pd.Series(median_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = strain_log_ratio.groupby(perturbation_pairs.transition).mean().T.sort_values('FT').assign(median_beta=median_beta)\n",
    "\n",
    "plt.scatter('median_beta', 'FT', c='TF', data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.summary(trace, var_names=['beta_abx_strain']).sort_values('hdi_3%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    trace.posterior.beta_abx_strain[:,:,0,42].values.flatten(),\n",
    "    trace.posterior.beta_abx_strain[:,:,0,44].values.flatten(),\n",
    "    c=trace.posterior.beta_abx_pooled.values.flatten(),\n",
    "    s=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}